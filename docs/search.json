[
  {
    "objectID": "HW5-Fitting Models.html",
    "href": "HW5-Fitting Models.html",
    "title": "HW5-Fitting Models",
    "section": "",
    "text": "The purpose of this homework is to get practice fitting models using the caret package."
  },
  {
    "objectID": "HW5-Fitting Models.html#task-1-conceptual-questions",
    "href": "HW5-Fitting Models.html#task-1-conceptual-questions",
    "title": "HW5-Fitting Models",
    "section": "Task 1: Conceptual Questions",
    "text": "Task 1: Conceptual Questions\n\nWhat is the purpose of using cross-validation when fitting a random forest model?\nCross-validation is essential when fitting a random forest model because it provides a robust way to evaluate the model’s performance, prevents overfitting, helps in hyperparameter tuning, and ensures the model’s reliability and generalizability.\nDescribe the bagged tree algorithm.\nGenerate new training samples (bootstrap sampling) with replacement. Training individual tree and aggregate pridictions. It reduces overfitting, improves accuracy, and provides robustness against noise and outliers, making it a powerful ensemble technique in machine learning.\nWhat is meant by a general linear model?\nA General Linear Model (GLM) is a framework that extends linear regression to model response variables with different types of distributions by using a link function to relate the mean of the response variable to the linear predictor. GLMs are versatile and can be used for various types of data, making them a powerful tool in statistical modeling and analysis.\nWhen fitting a multiple linear regression model, what does adding an interacion term do? That is , what does it allow the model to do differently as compared to when it is not included in the model?\nWhen fitting a multiple linear regression model, adding an interaction term allows the model to account for the combined effect of two (or more) predictor variables on the response variable. This can be particularly useful when the effect of one predictor variable on the response variable depends on the value of another predictor variable.\nWhy do we split our data into a training and test set?\nSplitting data into a training set and a test set is essential for assessing the model’s ability to generalize to new data, Preventing overfitting, providing an unbiased evaluation of model performance, avoiding data leakage, ensuring that the model’s performance is evaluated accurately and fairly.\n\n\nTask 2: Fitting Models\nHere we use the dataset called heart.csv. The data set gives information about whether or not someone has heart disease (HeartDisease =1 or =0) along with different measurements about that person’s health.\n\nQuick EDA / Data Preparation\nLoading required packages\n\nlibrary(readr)\nlibrary(caret)\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(randomForest)\n\nRead in the data first.\n\nheart_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/online/datasets/heart.csv\")\n\nRows: 918 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope\ndbl (7): Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak, HeartDisease\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter checking the data, I want to convert the character variables to factors. And also there is a numeric variable FastingBS (0/1) also need to be converted to factor. And then check on the missingness and summarize the data with respect to the relationships of the variables to HeartDisease.\n\n# convert character variables to factors\nheart_data &lt;- heart_data |&gt;\n  mutate(across(where(is.character), as.factor))\n\nheart_data$FastingBS &lt;- as.factor(heart_data$FastingBS)\n\n# check missing values\nsum(is.na(heart_data))\n\n[1] 0\n\n\nData set doesn’t contain any missing values. Then we do some summaries for both numerical and categorical variables.\n\n# summary the whole data frame\nsummary(heart_data)\n\n      Age        Sex     ChestPainType   RestingBP      Cholesterol   \n Min.   :28.00   F:193   ASY:496       Min.   :  0.0   Min.   :  0.0  \n 1st Qu.:47.00   M:725   ATA:173       1st Qu.:120.0   1st Qu.:173.2  \n Median :54.00           NAP:203       Median :130.0   Median :223.0  \n Mean   :53.51           TA : 46       Mean   :132.4   Mean   :198.8  \n 3rd Qu.:60.00                         3rd Qu.:140.0   3rd Qu.:267.0  \n Max.   :77.00                         Max.   :200.0   Max.   :603.0  \n FastingBS  RestingECG      MaxHR       ExerciseAngina    Oldpeak       \n 0:704     LVH   :188   Min.   : 60.0   N:547          Min.   :-2.6000  \n 1:214     Normal:552   1st Qu.:120.0   Y:371          1st Qu.: 0.0000  \n           ST    :178   Median :138.0                  Median : 0.6000  \n                        Mean   :136.8                  Mean   : 0.8874  \n                        3rd Qu.:156.0                  3rd Qu.: 1.5000  \n                        Max.   :202.0                  Max.   : 6.2000  \n ST_Slope    HeartDisease   \n Down: 63   Min.   :0.0000  \n Flat:460   1st Qu.:0.0000  \n Up  :395   Median :1.0000  \n            Mean   :0.5534  \n            3rd Qu.:1.0000  \n            Max.   :1.0000  \n\n\nCalculate and plot correlation between all numerical features.\n\nm= cor(select(heart_data,c(HeartDisease,Age,RestingBP,Cholesterol, MaxHR,Oldpeak)))\ncorrplot(m, method='color',\n         order='alphabet',\n         diag=FALSE,\n         col=COL2('PiYG')\n      )\n\n\n\n\n\n\n\n\nIt seems person with heart disease has some linear correlation with age, resting blood pressure, serum cholesterol, maximum heart rate and oldpeak.\nNow, check on categorical variables vs. HeartDisease\n\n# Function to summarize a categorical variable by HeartDisease\n\ncategory_summary &lt;- function(var){\n  heart_data |&gt;\n  count(HeartDisease, {{var}}) |&gt;\n  group_by(HeartDisease) |&gt;\n  mutate(percentage = n / sum(n) * 100)\n}\n\n# gender vs. heartdisease\ngender_summary &lt;- category_summary(Sex)\nprint(gender_summary)\n\n# A tibble: 4 × 4\n# Groups:   HeartDisease [2]\n  HeartDisease Sex       n percentage\n         &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;      &lt;dbl&gt;\n1            0 F       143      34.9 \n2            0 M       267      65.1 \n3            1 F        50       9.84\n4            1 M       458      90.2 \n\n# chestpain type vs. HeartDisease\nchestpain_summary &lt;-category_summary(ChestPainType)\nprint(chestpain_summary)\n\n# A tibble: 8 × 4\n# Groups:   HeartDisease [2]\n  HeartDisease ChestPainType     n percentage\n         &lt;dbl&gt; &lt;fct&gt;         &lt;int&gt;      &lt;dbl&gt;\n1            0 ASY             104      25.4 \n2            0 ATA             149      36.3 \n3            0 NAP             131      32.0 \n4            0 TA               26       6.34\n5            1 ASY             392      77.2 \n6            1 ATA              24       4.72\n7            1 NAP              72      14.2 \n8            1 TA               20       3.94\n\n# fastingBS vs HeartDisease\nfastingBS_summary &lt;- category_summary(FastingBS)\nprint(fastingBS_summary)\n\n# A tibble: 4 × 4\n# Groups:   HeartDisease [2]\n  HeartDisease FastingBS     n percentage\n         &lt;dbl&gt; &lt;fct&gt;     &lt;int&gt;      &lt;dbl&gt;\n1            0 0           366       89.3\n2            0 1            44       10.7\n3            1 0           338       66.5\n4            1 1           170       33.5\n\n# restingECG vs. HeartDisease\nresting_summary &lt;- category_summary(RestingECG)\nprint(resting_summary)\n\n# A tibble: 6 × 4\n# Groups:   HeartDisease [2]\n  HeartDisease RestingECG     n percentage\n         &lt;dbl&gt; &lt;fct&gt;      &lt;int&gt;      &lt;dbl&gt;\n1            0 LVH           82       20  \n2            0 Normal       267       65.1\n3            0 ST            61       14.9\n4            1 LVH          106       20.9\n5            1 Normal       285       56.1\n6            1 ST           117       23.0\n\n# exerciseangina vs heartdisease\nexerciseAngina_summary &lt;-category_summary(ExerciseAngina)\nprint(exerciseAngina_summary)\n\n# A tibble: 4 × 4\n# Groups:   HeartDisease [2]\n  HeartDisease ExerciseAngina     n percentage\n         &lt;dbl&gt; &lt;fct&gt;          &lt;int&gt;      &lt;dbl&gt;\n1            0 N                355       86.6\n2            0 Y                 55       13.4\n3            1 N                192       37.8\n4            1 Y                316       62.2\n\n\nFrom the above summary, Four categorical variables may be related to Heart Disease. Male may has high percentage with heart disease compared to female. Chest pain type with ASY, RestingECG with ST may has higher rate of heart disease. Heart disease is probably also related to exercise induced angina.\nNow we want to create a new variable which is a factor version of HeartDisease varible and remove the ST_slope and the original HeartDisease variable\n\nheart_data &lt;- heart_data |&gt;\n  mutate(HD=ifelse(HeartDisease==0,\"No\",\"Yes\")) |&gt;\n  mutate(HD=as.factor(HD))|&gt;\n  select(-HeartDisease,-ST_Slope) \n\nWe will create dummy columns corresponding to the Sex, ExerciseAngina, ChestPainType and RestingECG for use in KNN model.\n\n# Create dummy variables for categorical columns\ndummies &lt;- dummyVars(~ Sex + ChestPainType + RestingECG + ExerciseAngina, data = heart_data)\n\n# Generate the dummy columns\ndummy_data &lt;- predict(dummies, newdata = heart_data)\n\n# Convert the dummy data to a data frame\ndummy_data &lt;- as.data.frame(dummy_data)\n\n# Combine the dummy columns with the original data\nheart_data &lt;- cbind(heart_data, dummy_data)\n\nNow, let’s check the data structure\n\nstr(heart_data)\n\n'data.frame':   918 obs. of  22 variables:\n $ Age              : num  40 49 37 48 54 39 45 54 37 48 ...\n $ Sex              : Factor w/ 2 levels \"F\",\"M\": 2 1 2 1 2 2 1 2 2 1 ...\n $ ChestPainType    : Factor w/ 4 levels \"ASY\",\"ATA\",\"NAP\",..: 2 3 2 1 3 3 2 2 1 2 ...\n $ RestingBP        : num  140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol      : num  289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS        : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ RestingECG       : Factor w/ 3 levels \"LVH\",\"Normal\",..: 2 2 3 2 2 2 2 2 2 2 ...\n $ MaxHR            : num  172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina   : Factor w/ 2 levels \"N\",\"Y\": 1 1 1 2 1 1 1 1 2 1 ...\n $ Oldpeak          : num  0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HD               : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 1 1 1 1 2 1 ...\n $ Sex.F            : num  0 1 0 1 0 0 1 0 0 1 ...\n $ Sex.M            : num  1 0 1 0 1 1 0 1 1 0 ...\n $ ChestPainType.ASY: num  0 0 0 1 0 0 0 0 1 0 ...\n $ ChestPainType.ATA: num  1 0 1 0 0 0 1 1 0 1 ...\n $ ChestPainType.NAP: num  0 1 0 0 1 1 0 0 0 0 ...\n $ ChestPainType.TA : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG.LVH   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG.Normal: num  1 1 0 1 1 1 1 1 1 1 ...\n $ RestingECG.ST    : num  0 0 1 0 0 0 0 0 0 0 ...\n $ ExerciseAngina.N : num  1 1 1 0 1 1 1 1 0 1 ...\n $ ExerciseAngina.Y : num  0 0 0 1 0 0 0 0 1 0 ...\n\n\n\n\n\nSplit the data\nSplit the heart_data into training and test set with a 70:30 ratio.\n\nset.seed(5)\ntrainIndex &lt;-createDataPartition(heart_data$HD, p=0.7, list=FALSE)\ntrain_data &lt;-heart_data[trainIndex,]\ntest_data &lt;-heart_data[-trainIndex,]\n\n\n\nPreprocessing data\npreprocessing data\n\npre_proc_values &lt;- preProcess(train_data, method=c(\"center\",\"scale\"))\n\n# scaling and centralizing train and test data sets\ntrain_trans &lt;- predict(pre_proc_values, train_data)\ntest_trans &lt;- predict(pre_proc_values, test_data)\n\nTraining the kNN model, 10 fold cv, repeats =3. We need to standardize our data using caret’s preProcess() method.\n\ntrain.control &lt;- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nset.seed(123)\n\n# Remove unwanted columns\ntrain_data_knn &lt;- train_data |&gt;\n  select(-Sex, -ChestPainType, -FastingBS, -RestingECG, -ExerciseAngina)\n\n# Create the tuning grid for k values from 1 to 40\ntune_grid &lt;- expand.grid(k = 1:40)\n\nknn_model &lt;- train(HD~ ., \n                   data=train_data_knn, \n                   method=\"knn\", \n                   trControl=train.control,\n                   preProcess=c('center','scale'),\n                   tuneGrid=tune_grid)\n\nknn_model\n\nk-Nearest Neighbors \n\n643 samples\n 16 predictor\n  2 classes: 'No', 'Yes' \n\nPre-processing: centered (16), scaled (16) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 578, 579, 578, 579, 579, 579, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7751759  0.5479688\n   2  0.7813212  0.5594320\n   3  0.7907275  0.5777054\n   4  0.7979065  0.5927962\n   5  0.8109446  0.6198381\n   6  0.8056799  0.6093205\n   7  0.8020654  0.6019602\n   8  0.8062165  0.6101834\n   9  0.8035868  0.6048437\n  10  0.8057836  0.6089835\n  11  0.7995331  0.5963172\n  12  0.7948210  0.5867341\n  13  0.7958958  0.5891886\n  14  0.8005355  0.5983682\n  15  0.8015937  0.6003608\n  16  0.8062822  0.6093112\n  17  0.8026519  0.6023430\n  18  0.8005202  0.5982539\n  19  0.8088380  0.6142741\n  20  0.8119555  0.6205463\n  21  0.8134534  0.6238227\n  22  0.8088062  0.6138168\n  23  0.8088463  0.6142850\n  24  0.8124197  0.6217965\n  25  0.8088297  0.6141379\n  26  0.8114264  0.6191198\n  27  0.8072833  0.6107233\n  28  0.8124596  0.6207149\n  29  0.8057531  0.6070172\n  30  0.8124601  0.6198159\n  31  0.8104085  0.6162012\n  32  0.8166110  0.6284976\n  33  0.8181897  0.6311455\n  34  0.8166190  0.6281102\n  35  0.8191500  0.6331666\n  36  0.8191663  0.6330036\n  37  0.8197279  0.6340168\n  38  0.8202080  0.6349854\n  39  0.8181652  0.6307121\n  40  0.8181735  0.6308435\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 38.\n\n\nFrom the results, it automatically selects best k-value. Here, our training model is choosing k = 35 as its final value.\nLet’s plot the result\n\nplot(knn_model)\n\n\n\n\n\n\n\n\nAs it could be seen from the graph above, k=35 has the highest accuracy.\nNow, our model is trained with K value as 35. We are ready to predict classes for our test set. We can use predict() method.\n\ntest_pred &lt;- predict(knn_model, newdata = test_trans)\ntest_pred\n\n  [1] No  No  No  Yes No  No  No  Yes Yes No  No  No  No  No  Yes No  No  Yes\n [19] Yes No  No  No  Yes Yes No  No  Yes No  Yes Yes No  No  Yes No  No  Yes\n [37] Yes No  Yes No  No  Yes Yes No  Yes Yes No  No  Yes Yes Yes No  Yes No \n [55] No  No  No  Yes No  No  Yes No  No  Yes Yes No  Yes No  No  No  No  Yes\n [73] Yes Yes Yes Yes Yes No  Yes No  No  Yes Yes No  No  No  No  Yes No  Yes\n [91] Yes Yes Yes Yes Yes Yes Yes No  Yes Yes No  Yes Yes Yes Yes No  Yes Yes\n[109] No  Yes No  Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes\n[127] No  Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes\n[145] Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes\n[163] Yes Yes Yes Yes No  No  Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes\n[181] Yes No  No  No  Yes Yes Yes Yes No  No  Yes Yes Yes No  Yes No  Yes No \n[199] No  No  No  Yes No  No  Yes No  No  No  Yes No  No  Yes No  Yes Yes Yes\n[217] No  No  Yes No  Yes Yes No  Yes No  Yes Yes Yes Yes Yes Yes No  No  No \n[235] No  No  Yes Yes Yes No  Yes Yes No  Yes No  No  No  No  Yes No  No  No \n[253] Yes No  No  No  No  No  Yes No  Yes No  No  No  Yes No  No  Yes No  Yes\n[271] No  No  Yes No  No \nLevels: No Yes\n\n\nCheck how Accurately our knn model is working? Using confusionMatrix(), we can print statistics of our results. It shows that our model accuracy for test set is 76.36%.\n\nconfusionMatrix(test_pred, test_trans$HD)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   88  30\n       Yes  35 122\n                                          \n               Accuracy : 0.7636          \n                 95% CI : (0.7089, 0.8126)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : 3.04e-13        \n                                          \n                  Kappa : 0.5201          \n                                          \n Mcnemar's Test P-Value : 0.6198          \n                                          \n            Sensitivity : 0.7154          \n            Specificity : 0.8026          \n         Pos Pred Value : 0.7458          \n         Neg Pred Value : 0.7771          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3200          \n   Detection Prevalence : 0.4291          \n      Balanced Accuracy : 0.7590          \n                                          \n       'Positive' Class : No              \n                                          \n\n\n\n\nLogistic Regression\npreprocessing data\n\n#remove the dummy variables from train and test dataset\ntrain_log &lt;- train_data |&gt;\n  select(1:11)\ntest_log &lt;- test_data |&gt;\n  select(1:11)\n\npre_proc_values &lt;- preProcess(train_log, method=c(\"center\",\"scale\"))\n\n# scaling and centralizing train and test data sets\ntrain_trans &lt;- predict(pre_proc_values, train_log)\ntest_trans &lt;- predict(pre_proc_values, test_log)\n\n\n# Define train control for repeated CV\ntrain.control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\n# Set seed for reproducibility\nset.seed(123)\n\nLogistic Regression Model\nFitting logistic regression model 1 with all variables from train_trans data set except HD. (dummy variables already removed here)\n\n# Fit Model 1\nglm_model_1 &lt;- train(HD ~ ., \n                     data = train_trans,\n                     method = \"glm\", \n                     family = \"binomial\",\n                     trControl = train.control,\n                     metric=\"Accuracy\")\n\n# Print summary of Model 1\nsummary(glm_model_1)\n\n\nCall:\nNULL\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -0.442834   0.385857  -1.148  0.25111    \nAge               0.245449   0.136022   1.804  0.07116 .  \nSexM              1.398721   0.311016   4.497 6.88e-06 ***\nChestPainTypeATA -2.079358   0.350565  -5.931 3.00e-09 ***\nChestPainTypeNAP -1.537040   0.284982  -5.393 6.91e-08 ***\nChestPainTypeTA  -1.288983   0.479028  -2.691  0.00713 ** \nRestingBP         0.005698   0.118918   0.048  0.96178    \nCholesterol      -0.348418   0.138046  -2.524  0.01161 *  \nFastingBS1        1.332280   0.305616   4.359 1.30e-05 ***\nRestingECGNormal -0.244630   0.303653  -0.806  0.42046    \nRestingECGST     -0.500140   0.404013  -1.238  0.21574    \nMaxHR            -0.325816   0.142386  -2.288  0.02212 *  \nExerciseAnginaY   1.160546   0.273838   4.238 2.25e-05 ***\nOldpeak           0.688956   0.142257   4.843 1.28e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 883.97  on 642  degrees of freedom\nResidual deviance: 481.88  on 629  degrees of freedom\nAIC: 509.88\n\nNumber of Fisher Scoring iterations: 5\n\nglm_model_1\n\nGeneralized Linear Model \n\n643 samples\n 10 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 578, 579, 578, 579, 579, 579, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.8201684  0.6358332\n\n\nPredict on test data and Check how well this model 1 does on the test set using confusionMatrix(). We find the accuracy is 83.27%\n\ntest_pred_glm_1 &lt;- predict(glm_model_1, newdata=select(test_trans, -c(HD)), type='raw')\ntest_pred_glm_1\n\n  [1] No  Yes No  Yes No  No  No  Yes Yes No  No  No  No  No  Yes No  No  No \n [19] Yes Yes No  No  Yes No  No  No  No  No  Yes No  No  No  No  No  No  Yes\n [37] Yes No  No  No  No  Yes Yes No  Yes No  No  No  Yes Yes Yes No  Yes No \n [55] No  No  No  Yes No  No  Yes No  No  Yes Yes No  Yes No  No  No  No  No \n [73] Yes Yes Yes Yes Yes No  Yes No  No  Yes Yes No  No  No  No  Yes No  Yes\n [91] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes\n[109] Yes Yes No  Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes\n[127] Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes\n[145] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes\n[163] Yes Yes Yes Yes Yes No  No  No  Yes Yes Yes Yes Yes Yes Yes Yes No  Yes\n[181] Yes Yes No  No  Yes Yes Yes Yes No  No  Yes Yes Yes Yes Yes Yes No  No \n[199] No  No  No  Yes No  No  Yes No  No  No  Yes No  No  Yes No  Yes Yes No \n[217] No  No  Yes No  No  Yes Yes Yes No  Yes No  Yes Yes Yes Yes No  Yes Yes\n[235] Yes Yes Yes Yes Yes No  Yes Yes No  Yes No  No  No  No  Yes No  No  No \n[253] No  No  No  No  No  Yes No  No  Yes No  No  No  Yes No  No  Yes No  Yes\n[271] No  No  Yes Yes No \nLevels: No Yes\n\n# check how well this model does\nconfusionMatrix(test_pred_glm_1, test_trans$HD)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   97  20\n       Yes  26 132\n                                          \n               Accuracy : 0.8327          \n                 95% CI : (0.7833, 0.8749)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6601          \n                                          \n Mcnemar's Test P-Value : 0.461           \n                                          \n            Sensitivity : 0.7886          \n            Specificity : 0.8684          \n         Pos Pred Value : 0.8291          \n         Neg Pred Value : 0.8354          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3527          \n   Detection Prevalence : 0.4255          \n      Balanced Accuracy : 0.8285          \n                                          \n       'Positive' Class : No              \n                                          \n\n\nLogistic Regression Model 2: fiting a model with predictors: Age, Oldpeak, MaxHR, ChestPainType,ExerciseAngina,Cholesterol\n\n# Fit model 2\nglm_model_2 &lt;- train(HD ~ Age+MaxHR+Oldpeak+ExerciseAngina+Cholesterol, \n                     data = train_trans, \n                     method = \"glm\", \n                     family = \"binomial\",\n                     trControl = train.control,\n                     preProcess = c('center', 'scale'))\n\n# Print summary of Model 2\nsummary(glm_model_2)\n\n\nCall:\nNULL\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       0.4316     0.1081   3.994 6.51e-05 ***\nAge               0.2605     0.1139   2.286 0.022235 *  \nMaxHR            -0.4574     0.1187  -3.852 0.000117 ***\nOldpeak           0.7879     0.1290   6.109 1.00e-09 ***\nExerciseAnginaY   0.8038     0.1178   6.826 8.73e-12 ***\nCholesterol      -0.6024     0.1147  -5.252 1.50e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 883.97  on 642  degrees of freedom\nResidual deviance: 587.20  on 637  degrees of freedom\nAIC: 599.2\n\nNumber of Fisher Scoring iterations: 5\n\nglm_model_2\n\nGeneralized Linear Model \n\n643 samples\n  5 predictor\n  2 classes: 'No', 'Yes' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 578, 578, 578, 578, 580, 580, ... \nResampling results:\n\n  Accuracy  Kappa    \n  0.809847  0.6168373\n\n\nPredict on test data and Check how well this model does on the test set using confusionMatrix(). We find the accuracy is 80%\n\ntest_pred_glm_2 &lt;- predict(glm_model_2, newdata=select(test_trans, -c(HD)), type='raw')\ntest_pred_glm_2\n\n  [1] No  Yes No  Yes No  No  No  Yes Yes No  No  No  No  No  Yes Yes No  No \n [19] Yes Yes No  No  Yes No  No  No  No  No  Yes No  No  No  No  No  No  No \n [37] No  Yes No  Yes No  Yes Yes No  Yes No  No  No  Yes Yes Yes No  No  No \n [55] No  No  No  Yes No  No  Yes No  No  No  No  No  No  Yes No  No  No  No \n [73] Yes Yes Yes Yes Yes No  Yes No  No  Yes Yes No  No  Yes No  No  No  Yes\n [91] Yes Yes Yes Yes Yes No  Yes Yes Yes Yes No  No  Yes Yes Yes Yes Yes Yes\n[109] Yes Yes Yes Yes Yes Yes Yes No  No  Yes Yes Yes Yes Yes Yes Yes Yes Yes\n[127] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes\n[145] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes\n[163] Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes Yes Yes No  No \n[181] Yes Yes No  No  No  No  Yes Yes No  Yes Yes No  Yes Yes Yes Yes No  Yes\n[199] No  No  No  Yes No  No  Yes Yes Yes No  No  No  No  Yes No  Yes No  No \n[217] No  No  Yes No  No  Yes Yes Yes No  Yes No  Yes Yes Yes Yes No  Yes Yes\n[235] No  Yes Yes Yes No  No  No  Yes No  No  No  No  No  Yes Yes No  Yes No \n[253] Yes No  No  No  No  Yes No  No  Yes No  No  No  Yes No  No  Yes No  Yes\n[271] No  No  Yes Yes No \nLevels: No Yes\n\n# check how well this model does\nconfusionMatrix(test_pred_glm_2, test_trans$HD)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   94  26\n       Yes  29 126\n                                          \n               Accuracy : 0.8             \n                 95% CI : (0.7478, 0.8456)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.5946          \n                                          \n Mcnemar's Test P-Value : 0.7874          \n                                          \n            Sensitivity : 0.7642          \n            Specificity : 0.8289          \n         Pos Pred Value : 0.7833          \n         Neg Pred Value : 0.8129          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3418          \n   Detection Prevalence : 0.4364          \n      Balanced Accuracy : 0.7966          \n                                          \n       'Positive' Class : No              \n                                          \n\n\nLogistic Regression Model 3: fiting a model only with predictors Age and MaxHR interaction term,Oldpeak,ChestPainType,ExerciseAngina\n\n# Fit model 3\nglm_model_3 &lt;- train(HD ~ Age*MaxHR + Oldpeak+ Cholesterol+ExerciseAngina, \n                     data = train_trans, \n                     method = \"glm\", \n                     family = \"binomial\",\n                     trControl = train.control,\n                     preProcess = c('center', 'scale'))\n\n# Print summary of Model 3\nsummary(glm_model_3)\n\n\nCall:\nNULL\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      0.43624    0.10958   3.981 6.86e-05 ***\nAge              0.26617    0.11615   2.292 0.021925 *  \nMaxHR           -0.45963    0.11915  -3.857 0.000115 ***\nOldpeak          0.78812    0.12902   6.108 1.01e-09 ***\nCholesterol     -0.60093    0.11491  -5.230 1.70e-07 ***\nExerciseAnginaY  0.80455    0.11784   6.827 8.66e-12 ***\n`Age:MaxHR`     -0.03058    0.11818  -0.259 0.795848    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 883.97  on 642  degrees of freedom\nResidual deviance: 587.14  on 636  degrees of freedom\nAIC: 601.14\n\nNumber of Fisher Scoring iterations: 5\n\nglm_model_3\n\nGeneralized Linear Model \n\n643 samples\n  5 predictor\n  2 classes: 'No', 'Yes' \n\nPre-processing: centered (6), scaled (6) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 578, 578, 579, 579, 579, 579, ... \nResampling results:\n\n  Accuracy   Kappa   \n  0.8075515  0.612715\n\n\nPredict on test data and Check how well this model does on the test set using confusionMatrix(). We find the accuracy is 79.64%\n\ntest_pred_glm_3 &lt;- predict(glm_model_3, newdata=select(test_trans, -c(HD)), type='raw')\ntest_pred_glm_3\n\n  [1] No  Yes No  Yes No  No  No  Yes Yes No  No  No  No  No  Yes Yes No  No \n [19] Yes Yes No  No  Yes No  No  No  No  No  Yes No  No  No  No  No  No  No \n [37] No  Yes No  Yes No  Yes Yes No  Yes No  No  No  Yes Yes Yes No  No  No \n [55] No  No  No  Yes No  No  Yes No  No  No  No  No  No  Yes No  No  No  No \n [73] Yes Yes Yes Yes Yes No  Yes No  No  Yes Yes No  No  Yes No  No  No  Yes\n [91] Yes Yes No  Yes Yes No  Yes Yes Yes Yes No  No  Yes Yes Yes Yes Yes Yes\n[109] Yes Yes Yes Yes Yes Yes Yes No  No  Yes Yes Yes Yes Yes Yes Yes Yes Yes\n[127] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes\n[145] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes\n[163] Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes Yes Yes No  No \n[181] Yes Yes No  No  No  No  Yes Yes No  Yes Yes No  Yes Yes Yes Yes No  Yes\n[199] No  No  No  Yes No  No  Yes Yes Yes No  No  No  No  Yes No  Yes No  No \n[217] No  No  Yes No  No  Yes Yes Yes No  Yes No  Yes Yes Yes Yes No  Yes Yes\n[235] No  Yes Yes Yes No  No  No  Yes No  No  No  No  No  Yes Yes No  Yes No \n[253] Yes No  No  No  No  Yes No  No  Yes No  No  No  Yes No  No  Yes No  Yes\n[271] No  No  Yes Yes No \nLevels: No Yes\n\n# check how well this model does\nconfusionMatrix(test_pred_glm_3, test_trans$HD)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   94  27\n       Yes  29 125\n                                          \n               Accuracy : 0.7964          \n                 95% CI : (0.7439, 0.8424)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.5875          \n                                          \n Mcnemar's Test P-Value : 0.8937          \n                                          \n            Sensitivity : 0.7642          \n            Specificity : 0.8224          \n         Pos Pred Value : 0.7769          \n         Neg Pred Value : 0.8117          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3418          \n   Detection Prevalence : 0.4400          \n      Balanced Accuracy : 0.7933          \n                                          \n       'Positive' Class : No              \n                                          \n\n\nFrom the above three logistic regression model, the best model of them is the model with full predictors\nPredict on test data and Check how well this model 1 does on the test set using confusionMatrix(). We find the accuracy is 83.27%\n\ntest_pred_glm_1 &lt;- predict(glm_model_1, newdata=select(test_trans, -c(HD)), type='raw')\ntest_pred_glm_1\n\n  [1] No  Yes No  Yes No  No  No  Yes Yes No  No  No  No  No  Yes No  No  No \n [19] Yes Yes No  No  Yes No  No  No  No  No  Yes No  No  No  No  No  No  Yes\n [37] Yes No  No  No  No  Yes Yes No  Yes No  No  No  Yes Yes Yes No  Yes No \n [55] No  No  No  Yes No  No  Yes No  No  Yes Yes No  Yes No  No  No  No  No \n [73] Yes Yes Yes Yes Yes No  Yes No  No  Yes Yes No  No  No  No  Yes No  Yes\n [91] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes\n[109] Yes Yes No  Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes\n[127] Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes\n[145] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes\n[163] Yes Yes Yes Yes Yes No  No  No  Yes Yes Yes Yes Yes Yes Yes Yes No  Yes\n[181] Yes Yes No  No  Yes Yes Yes Yes No  No  Yes Yes Yes Yes Yes Yes No  No \n[199] No  No  No  Yes No  No  Yes No  No  No  Yes No  No  Yes No  Yes Yes No \n[217] No  No  Yes No  No  Yes Yes Yes No  Yes No  Yes Yes Yes Yes No  Yes Yes\n[235] Yes Yes Yes Yes Yes No  Yes Yes No  Yes No  No  No  No  Yes No  No  No \n[253] No  No  No  No  No  Yes No  No  Yes No  No  No  Yes No  No  Yes No  Yes\n[271] No  No  Yes Yes No \nLevels: No Yes\n\n# check how well this model does\nconfusionMatrix(test_pred_glm_1, test_trans$HD)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   97  20\n       Yes  26 132\n                                          \n               Accuracy : 0.8327          \n                 95% CI : (0.7833, 0.8749)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6601          \n                                          \n Mcnemar's Test P-Value : 0.461           \n                                          \n            Sensitivity : 0.7886          \n            Specificity : 0.8684          \n         Pos Pred Value : 0.8291          \n         Neg Pred Value : 0.8354          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3527          \n   Detection Prevalence : 0.4255          \n      Balanced Accuracy : 0.8285          \n                                          \n       'Positive' Class : No              \n                                          \n\n\n\n\nTree Models\nClassification tree model\n\n# Define train control for repeated CV\ntrain.control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\n# Define the tuning grid for cp\ntune_grid &lt;- expand.grid(cp = seq(0, 0.1, by = 0.001))\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Fit the classification tree model using rpart\nclassification_tree_model &lt;- train(HD ~ Age+MaxHR+Oldpeak+ExerciseAngina+Cholesterol, \n                    data = train_trans, \n                    method = \"rpart\", \n                    trControl = train.control,\n                    tuneGrid = tune_grid,\n                    preProcess = c('center', 'scale'))\n\n# Print the summary of the fitted model\nprint(classification_tree_model)\n\nCART \n\n643 samples\n  5 predictor\n  2 classes: 'No', 'Yes' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 578, 579, 578, 579, 579, 579, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.7812954  0.5562248\n  0.001  0.7880753  0.5699020\n  0.002  0.7942532  0.5826704\n  0.003  0.7947820  0.5840292\n  0.004  0.7989489  0.5929036\n  0.005  0.7989489  0.5929036\n  0.006  0.7989409  0.5928546\n  0.007  0.8004874  0.5959328\n  0.008  0.7973862  0.5903817\n  0.009  0.7973862  0.5903817\n  0.010  0.7994537  0.5945527\n  0.011  0.7994537  0.5945527\n  0.012  0.7978992  0.5915408\n  0.013  0.7978992  0.5908689\n  0.014  0.7952785  0.5846323\n  0.015  0.7957994  0.5858795\n  0.016  0.7927064  0.5794447\n  0.017  0.7927064  0.5794447\n  0.018  0.7833710  0.5618118\n  0.019  0.7833710  0.5618118\n  0.020  0.7776408  0.5491216\n  0.021  0.7776408  0.5491216\n  0.022  0.7735142  0.5427478\n  0.023  0.7735142  0.5427478\n  0.024  0.7740593  0.5437289\n  0.025  0.7740593  0.5437289\n  0.026  0.7740673  0.5441850\n  0.027  0.7740673  0.5441850\n  0.028  0.7771763  0.5521269\n  0.029  0.7766555  0.5514626\n  0.030  0.7781939  0.5549239\n  0.031  0.7803103  0.5596890\n  0.032  0.7823616  0.5641805\n  0.033  0.7823616  0.5641805\n  0.034  0.7823616  0.5641805\n  0.035  0.7823616  0.5641805\n  0.036  0.7823616  0.5641805\n  0.037  0.7823616  0.5641805\n  0.038  0.7823616  0.5641805\n  0.039  0.7828825  0.5658989\n  0.040  0.7828825  0.5658989\n  0.041  0.7828825  0.5658989\n  0.042  0.7828825  0.5658989\n  0.043  0.7828825  0.5658989\n  0.044  0.7828825  0.5658989\n  0.045  0.7828825  0.5658989\n  0.046  0.7828825  0.5658989\n  0.047  0.7828825  0.5658989\n  0.048  0.7828825  0.5658989\n  0.049  0.7828825  0.5658989\n  0.050  0.7828825  0.5658989\n  0.051  0.7828825  0.5658989\n  0.052  0.7828825  0.5658989\n  0.053  0.7828825  0.5658989\n  0.054  0.7828825  0.5658989\n  0.055  0.7828825  0.5658989\n  0.056  0.7828825  0.5658989\n  0.057  0.7828825  0.5658989\n  0.058  0.7828825  0.5658989\n  0.059  0.7828825  0.5658989\n  0.060  0.7828825  0.5658989\n  0.061  0.7828825  0.5658989\n  0.062  0.7828825  0.5658989\n  0.063  0.7828825  0.5658989\n  0.064  0.7828825  0.5658989\n  0.065  0.7828825  0.5658989\n  0.066  0.7828825  0.5658989\n  0.067  0.7828825  0.5658989\n  0.068  0.7828825  0.5658989\n  0.069  0.7828825  0.5658989\n  0.070  0.7828825  0.5658989\n  0.071  0.7828825  0.5658989\n  0.072  0.7828825  0.5658989\n  0.073  0.7828825  0.5658989\n  0.074  0.7828825  0.5658989\n  0.075  0.7828825  0.5658989\n  0.076  0.7828825  0.5658989\n  0.077  0.7828825  0.5658989\n  0.078  0.7828825  0.5658989\n  0.079  0.7828825  0.5658989\n  0.080  0.7828825  0.5658989\n  0.081  0.7828825  0.5658989\n  0.082  0.7828825  0.5658989\n  0.083  0.7828825  0.5658989\n  0.084  0.7828825  0.5658989\n  0.085  0.7828825  0.5658989\n  0.086  0.7828825  0.5658989\n  0.087  0.7828825  0.5658989\n  0.088  0.7828825  0.5658989\n  0.089  0.7828825  0.5658989\n  0.090  0.7828825  0.5658989\n  0.091  0.7828825  0.5658989\n  0.092  0.7828825  0.5658989\n  0.093  0.7828825  0.5658989\n  0.094  0.7828825  0.5658989\n  0.095  0.7828825  0.5658989\n  0.096  0.7828825  0.5658989\n  0.097  0.7828825  0.5658989\n  0.098  0.7828825  0.5658989\n  0.099  0.7828825  0.5658989\n  0.100  0.7828825  0.5658989\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.007.\n\n\n\n# Make predictions on the test set\ntest_pred_class_tree &lt;- predict(classification_tree_model, test_trans)\n\n# Evaluate the model performance on the test set using confusionMatrix\nconf_matrix_class_tree &lt;- confusionMatrix(test_pred_class_tree, test_trans$HD)\nconf_matrix_class_tree\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   89  25\n       Yes  34 127\n                                          \n               Accuracy : 0.7855          \n                 95% CI : (0.7322, 0.8325)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : 6.355e-16       \n                                          \n                  Kappa : 0.563           \n                                          \n Mcnemar's Test P-Value : 0.2976          \n                                          \n            Sensitivity : 0.7236          \n            Specificity : 0.8355          \n         Pos Pred Value : 0.7807          \n         Neg Pred Value : 0.7888          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3236          \n   Detection Prevalence : 0.4145          \n      Balanced Accuracy : 0.7796          \n                                          \n       'Positive' Class : No              \n                                          \n\n\nRandom Forest\n\n# Determine the number of predictors\nnum_predictors &lt;- ncol(train_trans) - 1  # subtract 1 for the response variable\n\n# Define the tuning grid for mtry\ntune_grid &lt;- expand.grid(mtry = 1:num_predictors)\n\n# Fit the random forest model using rf\nrf_model &lt;- train(HD ~ ., \n                  data = train_trans, \n                  method = \"rf\", \n                  trControl = train.control,\n                  tuneGrid = tune_grid,\n                  preProcess = c('center', 'scale'))\n\n# Print the summary of the fitted model\nprint(rf_model)\n\nRandom Forest \n\n643 samples\n 10 predictor\n  2 classes: 'No', 'Yes' \n\nPre-processing: centered (13), scaled (13) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 578, 578, 578, 578, 580, 580, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   1    0.8289831  0.6529171\n   2    0.8212179  0.6386347\n   3    0.8160251  0.6273581\n   4    0.8134365  0.6223002\n   5    0.8108892  0.6170886\n   6    0.8072017  0.6096151\n   7    0.8041581  0.6034965\n   8    0.8036205  0.6024262\n   9    0.7989485  0.5931307\n  10    0.7989891  0.5931999\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 1.\n\n\n\n# Make predictions on the test set\ntest_pred_rf &lt;- predict(rf_model, test_trans)\n\n# Evaluate the model performance on the test set using confusionMatrix\nconf_matrix_rf &lt;- confusionMatrix(test_pred_rf, test_trans$HD)\nprint(conf_matrix_rf)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   96  17\n       Yes  27 135\n                                          \n               Accuracy : 0.84            \n                 95% CI : (0.7912, 0.8813)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6739          \n                                          \n Mcnemar's Test P-Value : 0.1748          \n                                          \n            Sensitivity : 0.7805          \n            Specificity : 0.8882          \n         Pos Pred Value : 0.8496          \n         Neg Pred Value : 0.8333          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3491          \n   Detection Prevalence : 0.4109          \n      Balanced Accuracy : 0.8343          \n                                          \n       'Positive' Class : No              \n                                          \n\n\nBoosted Tree\n\n# Define the tuning grid\ntune_grid &lt;- expand.grid(\n  n.trees = c(25, 50, 100, 200),\n  interaction.depth = c(1, 2, 3),\n  shrinkage = 0.1,\n  n.minobsinnode = 10\n)\n\n# Define train control for repeated CV\ntrain.control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Fit the boosted tree model using gbm\ngbm_model &lt;- train(HD ~ ., \n                   data = train_trans, \n                   method = \"gbm\", \n                   trControl = train.control,\n                   tuneGrid = tune_grid,\n                   verbose = FALSE)\n\n# Print the summary of the fitted model\nprint(gbm_model)\n\nStochastic Gradient Boosting \n\n643 samples\n 10 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 578, 579, 578, 579, 579, 579, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.8062654  0.6070407\n  1                   50      0.8259058  0.6470879\n  1                  100      0.8248399  0.6455533\n  1                  200      0.8237734  0.6433587\n  2                   25      0.8206559  0.6359390\n  2                   50      0.8300081  0.6559006\n  2                  100      0.8320429  0.6600643\n  2                  200      0.8232037  0.6423344\n  3                   25      0.8222760  0.6397999\n  3                   50      0.8243754  0.6448427\n  3                  100      0.8305122  0.6565533\n  3                  200      0.8227315  0.6407222\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 100, interaction.depth =\n 2, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\nThe optimal model parameters were found to be n.trees = 100, interaction.depth = 2, shrinkage = 0.1, and n.minobsinnode = 10. This model is expected to provide the best performance based on the accuracy metric.\nEvaluate the Boosted Tree Model Using Confusion Matrix\n\n# Make predictions on the test set\ntest_pred_gbm &lt;- predict(gbm_model, test_trans)\n\n# Evaluate the model performance on the test set using confusionMatrix\nconf_matrix_gbm &lt;- confusionMatrix(test_pred_gbm, test_trans$HD)\nprint(conf_matrix_gbm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No   97  23\n       Yes  26 129\n                                          \n               Accuracy : 0.8218          \n                 95% CI : (0.7714, 0.8652)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6388          \n                                          \n Mcnemar's Test P-Value : 0.7751          \n                                          \n            Sensitivity : 0.7886          \n            Specificity : 0.8487          \n         Pos Pred Value : 0.8083          \n         Neg Pred Value : 0.8323          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3527          \n   Detection Prevalence : 0.4364          \n      Balanced Accuracy : 0.8187          \n                                          \n       'Positive' Class : No              \n                                          \n\n\nAmong the Tree Models, accuracy of classfication tree model, random forest, boosted tree model is 0.7855, 0.84, 0.8218 respectively. Thus, the random forest model did the best job among these three models.\n\n\nModel Comparison\nAccuracy of kNN model on the test set is 0.8286. Accuracy of Logistic Regression Model 1 on the test set is 0.8327. Accuracy of the best Tree Model is random forest model which has an accuracy of 0.84. In conclusion, random forest model did the best job in terms of accuracy on the test set."
  }
]